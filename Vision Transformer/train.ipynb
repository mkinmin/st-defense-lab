{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patches embedding\n",
    "##### 1. Conv layer를 활용하여 패치 나누기 \n",
    "(논문의 Hybrid Architecture로 언급)\n",
    "##### 2. Trainable linear projection을 통해 x의 각 패치를 flatten한 벡터를 D차원으로 변환한 후, 이를 패치 임베딩으로 사용\n",
    "\n",
    "##### 3. Learnable 임베딩과 패치 임베딩에 learnable position 임베딩을 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, input_channels: int=3, patch_size: int=2,\n",
    "                 embedding_size: int=768, img_size: int=224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.partition = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, embedding_size,\n",
    "              kernel_size=patch_size, stride=patch_size),   # torch.Size([1, 768, 14, 14])\n",
    "                 Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, embedding_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, embedding_size))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.partition(x)   # flatten 과정\n",
    "        cls_token = repeat(self.cls_token, '() n e -> b n e', b=b )   # cls_token을 배치 크기에 맞춰 복제\n",
    "        #print(cls_token.shape)\n",
    "        x = torch.cat([cls_token, x], dim=1)   # ([1, 196 + 1, 768])  dim=1로 설정해서 cls_token을 패치 앞에 추가\n",
    "        #print(x.shape)\n",
    "        x += self.positions   # position embedding 더하기\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer encoder\n",
    "##### 4. 임베딩을 Transformer encode에 input으로 넣어 마지막 layer에서 class embedding에 대한 output인 image representation을 도출\n",
    "\n",
    "#### 1. Multi-Head Attention\n",
    "패치에 대해 self-attention 메커니즘 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_size: int=768,\n",
    "                 num_heads: int=8,\n",
    "                 dropout: float=0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(embedding_size, embedding_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(embedding_size, embedding_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        score = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32),min\n",
    "            score.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        scaling = self.embedding_size ** (1/2)\n",
    "        attention = F.softmax(score / scaling, dim=-1)\n",
    "        attention = self.att_drop(attention)\n",
    "        \n",
    "        output = torch.einsum('bhal, bhlv -> bhav', attention, values)\n",
    "        output = rearrange(output, \"b h n d -> b n (h d)\")\n",
    "        output = self.projection(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transformer Encoder Block\n",
    "MLP(Feed Forward) 블럭을 만들어주도 Multi Head Attention을 하나로 묶음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, embedding_size: int, expansion: int=4, drop_p: float=0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_size, expansion * embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * embedding_size, embedding_size)\n",
    "        )\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self, embedding_size: int=768,\n",
    "                 drop_p: float=0.,\n",
    "                 forward_expansion: int=4,\n",
    "                 forward_drop_p: float=0.,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(embedding_size),\n",
    "                MultiHeadAttention(embedding_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(embedding_size),\n",
    "                FeedForwardBlock(embedding_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            ))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Head\n",
    "##### 5. classification을 위한 MLP Head 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int=12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, embedding_size: int=768,\n",
    "                 n_classes: int=1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(embedding_size),\n",
    "            nn.Linear(embedding_size, n_classes)\n",
    "        )\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 input_channels: int=3,\n",
    "                 patch_size: int=2,\n",
    "                 embedding_size: int=768,\n",
    "                 img_size: int=224,\n",
    "                 depth: int=12,\n",
    "                 n_classes: int=1000, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(input_channels, patch_size, embedding_size, img_size),\n",
    "            TransformerEncoder(depth, embedding_size=embedding_size, **kwargs),\n",
    "            ClassificationHead(embedding_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:25<00:00, 6544307.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [04:26<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 3.9902, Train Acc: 0.0853, Val Loss: 3.6062, Val Acc: 0.1362\n",
      "Validation loss decreased (inf --> 3.606250).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 176/625 [01:16<03:15,  2.30it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR100, ImageNet\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn  # nn 모듈 추가\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt  # 플롯팅을 위해 추가\n",
    "import numpy as np  # 플롯팅을 위해 추가\n",
    "\n",
    "# Training settings\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LR = 1e-4\n",
    "CHECKPOINT_DIR = './checkpoints/'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PATIENCE = 5  # Early Stopping을 위한 patience 설정\n",
    "\n",
    "\"\"\"\n",
    "# CIFAR-10 dataset loading with transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # ViT expects 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])  # CIFAR-10 normalization\n",
    "])\n",
    "\"\"\"\n",
    "# CIFAR-100 dataset loading with transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # ViT expects 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])  # CIFAR-100 normalization values\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "train_dataset = CIFAR100(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR100(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Split train into train/validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader for train, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model (ViT 모델이 정의되어 있다고 가정합니다)\n",
    "# ViT 클래스를 임포트하거나 정의해야 합니다.\n",
    "# from vit_model import ViT  # 예시\n",
    "\n",
    "model = ViT(input_channels=3, patch_size=16, embedding_size=768, img_size=224, depth=12, n_classes=100).to(DEVICE)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Function to save model checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, path):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        os.makedirs(CHECKPOINT_DIR)\n",
    "    torch.save(state, path)\n",
    "\n",
    "# Early Stopping 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta  # 개선된 것으로 간주되는 최소 변화량\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        score = -val_loss  # 손실이 감소하면 score는 증가\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, epoch):\n",
    "        '''검증 손실이 감소하면 모델을 저장합니다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/best_model_epoch_{epoch+1}.pth')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Training and validation loop with Early Stopping\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "early_stopping = EarlyStopping(patience=PATIENCE, verbose=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    for imgs, labels in tqdm(train_loader):\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, f'{CHECKPOINT_DIR}/vit_epoch_{epoch+1}.pth')\n",
    "    \n",
    "    # Early Stopping 체크\n",
    "    early_stopping(val_loss, model, epoch)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    \n",
    "    # Adjust learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# Plotting the training loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, len(train_losses), step=5))  # 가로축을 5단위로 표시\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, len(train_accs), step=5))  # 가로축을 5단위로 표시\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Load the best model (Early Stopping으로 저장된 모델)\n",
    "model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/best_model_epoch_{epoch+1}.pth'))\n",
    "\n",
    "# Test the model on test dataset\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defense_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
