{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 2.4.1+cu121 (NVIDIA GeForce RTX 4070)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\kimin\\anaconda3\\envs\\defense_lab\\lib\\site-packages (from opencv-python) (1.26.3)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "# !pip install matplotlib\n",
    "# ! pip install einops\n",
    "# !pip install torchsummary\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as F\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops import rearrange, repeat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Project input to patches\n",
    "### 방법1. 입력 이미지를 패치로 나누어주기 (linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([1, 3, 224, 224])\n",
      "patches : torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224)   # randn함수로 주어진 크기(shape)의 tensor를 생성하고 \n",
    "print('x :', x.shape)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# import cv2\n",
    "\n",
    "# batch_size = 1\n",
    "# img_path = \"C:/Users/kimin/st_defense_lab/st-defense-lab/Vision Transformer/test_images.jpg\"\n",
    "# x = cv2.imread(img_path)\n",
    "# x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "# x = repeat(x, 'h w c -> b c h w', b=batch_size)\n",
    "# print('x :', x.shape)\n",
    "#-------------------------------------------------\n",
    "\n",
    "patch_size = 16   # 16 x 16 사이즈 패치 \n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)',   # h: height, w: width, c: channel, s1,s2: patch_size\n",
    "                    s1=patch_size, s2=patch_size)\n",
    "print('patches :', patches.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법2. Conv layer를 활용하여 패치 나누기 \n",
    "\n",
    "\n",
    "이 방법은 논문의 Hybrid Architecture로 언급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "from torchsummary import summary\n",
    "\n",
    "patch_size = 16\n",
    "input_channels = 3\n",
    "embedding_size = 768   # channel * patch_size * patch_size\n",
    "\n",
    "partition = nn.Sequential(\n",
    "    nn.Conv2d(input_channels, embedding_size,\n",
    "              kernel_size=patch_size, stride=patch_size),   # torch.Size([1, 768, 14, 14])\n",
    "    Rearrange('b e (h) (w) -> b (h w) e'))\n",
    "\n",
    "summary(partition, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. Patches embedding\n",
    "Trainable linear projection을 통해 x의 각 패치를 flatten한 벡터를 D차원으로 변환한 후, 이를 패치 임베딩으로 사용\n",
    "\n",
    "Learnable 임베딩과 패치 임베딩에 learnable position 임베딩을 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected X shape : torch.Size([1, 196, 768])\n",
      "Cls Shape : torch.Size([1, 1, 768]) , Pos Shape : torch.Size([197, 768])\n",
      "Repeated Cls shape : torch.Size([1, 1, 768])\n",
      "tensor([[[ 0.4201, -0.6858, -0.4151,  ..., -1.1514,  0.7832, -1.1986],\n",
      "         [ 0.2705,  0.1865, -0.2750,  ...,  1.0042,  1.1362,  0.0793],\n",
      "         [ 0.1894,  0.5829,  0.4339,  ..., -0.4575, -0.3327, -0.3140],\n",
      "         ...,\n",
      "         [-0.3102, -0.2404, -0.6107,  ...,  0.5053, -0.8523, -0.5888],\n",
      "         [-0.1326,  0.3110,  0.0939,  ..., -0.2882,  0.4311,  1.4987],\n",
      "         [-0.3215, -0.1747, -0.4515,  ..., -0.7191, -0.3624,  0.6005]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "output : torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 16\n",
    "embedding_size = 768\n",
    "img_size = 224\n",
    "\n",
    "# 이미지를 패치사이즈로 나누고 flatten\n",
    "projected_x = partition(x)   # flatten 과정\n",
    "print('Projected X shape :', projected_x.shape)   # (배치 크기, 패치 수, 임베딩 크기)\n",
    "\n",
    "# cls_token과 position embedding parameter 정의\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, embedding_size))   # class token으로 Transformer 기반 모델에서 입력 시퀀스 앞에 추가되는 벡터 (논문 이미지에서 *에 해당하는 부분)\n",
    "positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, embedding_size))   # 각 패치와 클래스 토큰에 대한 positional embedding을 위한 trainable한 parameter (14 x 14는 패치의 개수, +1은 CLS token)\n",
    "print('Cls Shape :', cls_token.shape, ', Pos Shape :', positions.shape) \n",
    "\n",
    "# cls_token을 반복하여 batch_size의 크기와 맞춰줌\n",
    "batch_size = 1\n",
    "cls_token = repeat(cls_token, '() n e -> b n e', b=batch_size)   # cls_token을 배치 크기에 맞춰 복제\n",
    "print('Repeated Cls shape :', cls_token.shape)   # (배치 크기, cls_token 개수, 임베딩 크기)\n",
    "\n",
    "# cls_token과 projected_x를 concatenate\n",
    "concat = torch.cat([cls_token, projected_x], dim=1)   # ([1, 196 + 1, 768])  dim=1로 설정해서 cls_token을 패치 앞에 추가\n",
    "\n",
    "# position embedding을 더해줌\n",
    "concat += positions   # 패치와 cls_token에 대한 위치 정보 더함 \n",
    "print('output :', concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, input_channels: int=3, patch_size: int=16,\n",
    "                 embedding_size: int=768, img_size: int=224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.partition = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, embedding_size,\n",
    "              kernel_size=patch_size, stride=patch_size),   # torch.Size([1, 768, 14, 14])\n",
    "                 Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, embedding_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, embedding_size))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.partition(x)\n",
    "        cls_token = repeat(self.cls_token, '() n e -> b n e', b=b )\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x += self.positions\n",
    "\n",
    "        return x\n",
    "\n",
    "PE = PatchEmbedding()\n",
    "summary(PE, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Transformer encoder\n",
    "임베딩을 Transformer encode에 input으로 넣어 마지막 layer에서 class embedding에 대한 output인 image representation을 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "패치에 대해 self-attention 메커니즘 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "keys = nn.Linear(embedding_size, embedding_size)\n",
    "queries = nn.Linear(embedding_size, embedding_size)\n",
    "values = nn.Linear(embedding_size, embedding_size)\n",
    "print(keys, queries, values)\n",
    "\n",
    "x = PE(x)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, embedding_size: int, expansion: int=4, drop_p: float=0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_size, expansion * embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * embedding_size, embedding_size)\n",
    "        )\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self, embedding_size: int=768,\n",
    "                 drop_p: float=0.,\n",
    "                 forward_expansion: int=4,\n",
    "                 forward_drop_p: float=0.,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(embedding_size),\n",
    "                MultiHeadAttention(embedding_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(embedding_size),\n",
    "                FeedForwardBlock(embedding_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            ))\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
