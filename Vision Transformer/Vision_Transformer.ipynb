{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 2.4.1+cu121 (NVIDIA GeForce RTX 4070 Ti SUPER)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kimin\\anaconda3\\envs\\defense_lab\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install matplotlib\n",
    "# ! pip install einops\n",
    "# !pip install torchsummary\n",
    "# !pip install opencv-python\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops import rearrange, repeat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Project input to patches\n",
    "### 방법1. 입력 이미지를 패치로 나누어주기 (linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([1, 3, 224, 224])\n",
      "patches : torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224) \n",
    "print('x :', x.shape)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# import cv2\n",
    "\n",
    "# batch_size = 1\n",
    "# img_path = \"C:/Users/kimin/st_defense_lab/st-defense-lab/Vision Transformer/test_images.jpg\"\n",
    "# x = cv2.imread(img_path)\n",
    "# x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "# x = repeat(x, 'h w c -> b c h w', b=batch_size)\n",
    "# print('x :', x.shape)\n",
    "#-------------------------------------------------\n",
    "\n",
    "patch_size = 16   # 16 x 16 사이즈 패치 \n",
    "#print(x[0][0][0])\n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)',   # h: height, w: width, c: channel, s1,s2: patch_size\n",
    "                    s1=patch_size, s2=patch_size)\n",
    "print('patches :', patches.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법2. Conv layer를 활용하여 패치 나누기 \n",
    "\n",
    "\n",
    "이 방법은 논문의 Hybrid Architecture로 언급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "from torchsummary import summary\n",
    "\n",
    "patch_size = 16\n",
    "input_channels = 3\n",
    "embedding_size = 768   # channel * patch_size * patch_size\n",
    "\n",
    "partition = nn.Sequential(\n",
    "    nn.Conv2d(input_channels, embedding_size,\n",
    "              kernel_size=patch_size, stride=patch_size),   # torch.Size([1, 768, 14, 14])\n",
    "    Rearrange('b e (h) (w) -> b (h w) e'))\n",
    "\n",
    "summary(partition, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. Patches embedding\n",
    "Trainable linear projection을 통해 x의 각 패치를 flatten한 벡터를 D차원으로 변환한 후, 이를 패치 임베딩으로 사용\n",
    "\n",
    "Learnable 임베딩과 패치 임베딩에 learnable position 임베딩을 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected X shape : torch.Size([1, 196, 768])\n",
      "Cls Shape : torch.Size([1, 1, 768]) , Pos Shape : torch.Size([197, 768])\n",
      "Repeated Cls shape : torch.Size([1, 1, 768])\n",
      "output : torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "patch_size = 16\n",
    "embedding_size = 768\n",
    "img_size = 224\n",
    "\n",
    "# 이미지를 패치사이즈로 나누고 flatten\n",
    "projected_x = partition(x)   # flatten 과정\n",
    "print('Projected X shape :', projected_x.shape)   # (배치 크기, 패치 수, 임베딩 크기)\n",
    "\n",
    "# cls_token과 position embedding parameter 정의\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, embedding_size))   # class token으로 Transformer 기반 모델에서 입력 시퀀스 앞에 추가되는 벡터 (논문 이미지에서 *에 해당하는 부분)\n",
    "positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, embedding_size))   # 각 패치와 클래스 토큰에 대한 positional embedding을 위한 trainable한 parameter (14 x 14는 패치의 개수, +1은 CLS token)\n",
    "print('Cls Shape :', cls_token.shape, ', Pos Shape :', positions.shape) \n",
    "\n",
    "# cls_token을 반복하여 batch_size의 크기와 맞춰줌\n",
    "batch_size = 1\n",
    "cls_token = repeat(cls_token, '() n e -> b n e', b=batch_size)   # cls_token을 배치 크기에 맞춰 복제\n",
    "print('Repeated Cls shape :', cls_token.shape)   # (배치 크기, cls_token 개수, 임베딩 크기)\n",
    "\n",
    "# cls_token과 projected_x를 concatenate\n",
    "concat = torch.cat([cls_token, projected_x], dim=1)   # ([1, 196 + 1, 768])  dim=1로 설정해서 cls_token을 패치 앞에 추가\n",
    "\n",
    "# position embedding을 더해줌\n",
    "concat += positions   # 패치와 cls_token에 대한 위치 정보 더함 \n",
    "print('output :', concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 768])\n",
      "torch.Size([2, 197, 768])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, input_channels: int=3, patch_size: int=16,\n",
    "                 embedding_size: int=768, img_size: int=224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.partition = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, embedding_size,\n",
    "              kernel_size=patch_size, stride=patch_size),   # torch.Size([1, 768, 14, 14])\n",
    "                 Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, embedding_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, embedding_size))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.partition(x)   # flatten 과정\n",
    "        cls_token = repeat(self.cls_token, '() n e -> b n e', b=b )   # cls_token을 배치 크기에 맞춰 복제\n",
    "        print(cls_token.shape)\n",
    "        x = torch.cat([cls_token, x], dim=1)   # ([1, 196 + 1, 768])  dim=1로 설정해서 cls_token을 패치 앞에 추가\n",
    "        print(x.shape)\n",
    "        x += self.positions   # position embedding 더하기\n",
    "\n",
    "        return x\n",
    "\n",
    "PE = PatchEmbedding()\n",
    "summary(PE, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Transformer encoder\n",
    "임베딩을 Transformer encode에 input으로 넣어 마지막 layer에서 class embedding에 대한 output인 image representation을 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "패치에 대해 self-attention 메커니즘 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 197, 768])\n",
      "torch.Size([1, 197, 768])\n",
      "shape : torch.Size([1, 8, 197, 96]) torch.Size([1, 8, 197, 96]) torch.Size([1, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 768\n",
    "num_heads = 8   # 8개의 head로 구성\n",
    "\n",
    "keys = nn.Linear(embedding_size, embedding_size)\n",
    "queries = nn.Linear(embedding_size, embedding_size)\n",
    "values = nn.Linear(embedding_size, embedding_size)\n",
    "#print(keys, queries, values)\n",
    "\n",
    "x = PE(x)   # position embedding 적용 \n",
    "print(queries(x).shape)   # 배치 크기, n(시퀀스 길이 -> 패치 수), 임베딩 크기\n",
    "queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads)   # batch_size(b), num_head(h), n(시퀀스 길이), head_dim(d) = emb_size/head(각 head의 차원)\n",
    "                                                                       # rearrange에서 \"b n (h d)\" 여기서 h d는 embedding_size를 두 개의 차원인 h와 d로 나누라는 의미\n",
    "                                                                       # embedding_size = h * d가 되므로, rearrange는 자동으로 embedding/num_heads를 d로 정의\n",
    "keys = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "values = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "\n",
    "print('shape :', queries.shape, keys.shape, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : torch.Size([1, 8, 197, 197])\n",
      "attention : torch.Size([1, 8, 197, 197])\n",
      "values : torch.Size([1, 8, 197, 96])\n",
      "output : torch.Size([1, 8, 197, 96])\n",
      "output2 : torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "score = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)   # Queries * Keys(transpose) 내적 => 'bhqd, bhkd -> bhqk' queries와 key 간의 내적을 계산하는 부분\n",
    "                                                            # b: batch_size, h: num_head, q: query_len(query의 시퀀스 길이), d: head_dim(각 헤드에서의 차원) => 'bhqd'\n",
    "                                                            # b: batch_size, h: num_head, q: key_len(key의 시퀀스 길이), d: head_dim(각 헤드에서의 차원)     => 'bhkd'\n",
    "                                                            # 'qd' * 'kd' 내적 -> 'bhqk'가 됨\n",
    "print('score :', score.shape)\n",
    "\n",
    "scaling = embedding_size ** (1/2)\n",
    "attention = F.softmax(score / scaling, dim=-1)\n",
    "print('attention :', attention.shape)\n",
    "print('values :', values.shape)\n",
    "\n",
    "output = torch.einsum('bhal, bhlv -> bhav', attention, values)   # attention * values 내적 \n",
    "print('output :', output.shape)\n",
    "\n",
    "output = rearrange(output, \"b h n d -> b n (h d)\")\n",
    "print('output2 :', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 768])\n",
      "torch.Size([8, 197, 768])\n",
      "torch.Size([8, 197, 768])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-2          [-1, 8, 197, 197]               0\n",
      "            Linear-3             [-1, 197, 768]         590,592\n",
      "================================================================\n",
      "Total params: 2,362,368\n",
      "Trainable params: 2,362,368\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.58\n",
      "Forward/backward pass size (MB): 6.99\n",
      "Params size (MB): 9.01\n",
      "Estimated Total Size (MB): 16.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_size: int=768,\n",
    "                 num_heads: int=8,\n",
    "                 dropout: float=0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(embedding_size, embedding_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(embedding_size, embedding_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        score = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32),min\n",
    "            score.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        scaling = self.embedding_size ** (1/2)\n",
    "        attention = F.softmax(score / scaling, dim=-1)\n",
    "        attention = self.att_drop(attention)\n",
    "        \n",
    "        output = torch.einsum('bhal, bhlv -> bhav', attention, values)\n",
    "        output = rearrange(output, \"b h n d -> b n (h d)\")\n",
    "        output = self.projection(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "x = torch.randn(8, 3, 224, 224)\n",
    "PE = PatchEmbedding()\n",
    "x = PE(x)\n",
    "print(x.shape)\n",
    "MHA = MultiHeadAttention()\n",
    "summary(MHA, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, embedding_size: int, expansion: int=4, drop_p: float=0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_size, expansion * embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * embedding_size, embedding_size)\n",
    "        )\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self, embedding_size: int=768,\n",
    "                 drop_p: float=0.,\n",
    "                 forward_expansion: int=4,\n",
    "                 forward_drop_p: float=0.,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(embedding_size),\n",
    "                MultiHeadAttention(embedding_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(embedding_size),\n",
    "                FeedForwardBlock(embedding_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            ))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4. Classification Head\n",
    "classification을 위한 MLP Head 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 768])\n",
      "torch.Size([2, 197, 768])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-8             [-1, 197, 768]               0\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "      ResidualAdd-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "      ResidualAdd-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-20          [-1, 8, 197, 197]               0\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-22             [-1, 197, 768]               0\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "      ResidualAdd-24             [-1, 197, 768]               0\n",
      "        LayerNorm-25             [-1, 197, 768]           1,536\n",
      "           Linear-26            [-1, 197, 3072]       2,362,368\n",
      "             GELU-27            [-1, 197, 3072]               0\n",
      "          Dropout-28            [-1, 197, 3072]               0\n",
      "           Linear-29             [-1, 197, 768]       2,360,064\n",
      "          Dropout-30             [-1, 197, 768]               0\n",
      "      ResidualAdd-31             [-1, 197, 768]               0\n",
      "        LayerNorm-32             [-1, 197, 768]           1,536\n",
      "           Linear-33            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-34          [-1, 8, 197, 197]               0\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-36             [-1, 197, 768]               0\n",
      "          Dropout-37             [-1, 197, 768]               0\n",
      "      ResidualAdd-38             [-1, 197, 768]               0\n",
      "        LayerNorm-39             [-1, 197, 768]           1,536\n",
      "           Linear-40            [-1, 197, 3072]       2,362,368\n",
      "             GELU-41            [-1, 197, 3072]               0\n",
      "          Dropout-42            [-1, 197, 3072]               0\n",
      "           Linear-43             [-1, 197, 768]       2,360,064\n",
      "          Dropout-44             [-1, 197, 768]               0\n",
      "      ResidualAdd-45             [-1, 197, 768]               0\n",
      "        LayerNorm-46             [-1, 197, 768]           1,536\n",
      "           Linear-47            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-48          [-1, 8, 197, 197]               0\n",
      "           Linear-49             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-50             [-1, 197, 768]               0\n",
      "          Dropout-51             [-1, 197, 768]               0\n",
      "      ResidualAdd-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 3072]       2,362,368\n",
      "             GELU-55            [-1, 197, 3072]               0\n",
      "          Dropout-56            [-1, 197, 3072]               0\n",
      "           Linear-57             [-1, 197, 768]       2,360,064\n",
      "          Dropout-58             [-1, 197, 768]               0\n",
      "      ResidualAdd-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-62          [-1, 8, 197, 197]               0\n",
      "           Linear-63             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-64             [-1, 197, 768]               0\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "      ResidualAdd-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 3072]       2,362,368\n",
      "             GELU-69            [-1, 197, 3072]               0\n",
      "          Dropout-70            [-1, 197, 3072]               0\n",
      "           Linear-71             [-1, 197, 768]       2,360,064\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "      ResidualAdd-73             [-1, 197, 768]               0\n",
      "        LayerNorm-74             [-1, 197, 768]           1,536\n",
      "           Linear-75            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-76          [-1, 8, 197, 197]               0\n",
      "           Linear-77             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-78             [-1, 197, 768]               0\n",
      "          Dropout-79             [-1, 197, 768]               0\n",
      "      ResidualAdd-80             [-1, 197, 768]               0\n",
      "        LayerNorm-81             [-1, 197, 768]           1,536\n",
      "           Linear-82            [-1, 197, 3072]       2,362,368\n",
      "             GELU-83            [-1, 197, 3072]               0\n",
      "          Dropout-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "      ResidualAdd-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-90          [-1, 8, 197, 197]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-92             [-1, 197, 768]               0\n",
      "          Dropout-93             [-1, 197, 768]               0\n",
      "      ResidualAdd-94             [-1, 197, 768]               0\n",
      "        LayerNorm-95             [-1, 197, 768]           1,536\n",
      "           Linear-96            [-1, 197, 3072]       2,362,368\n",
      "             GELU-97            [-1, 197, 3072]               0\n",
      "          Dropout-98            [-1, 197, 3072]               0\n",
      "           Linear-99             [-1, 197, 768]       2,360,064\n",
      "         Dropout-100             [-1, 197, 768]               0\n",
      "     ResidualAdd-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-104          [-1, 8, 197, 197]               0\n",
      "          Linear-105             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-106             [-1, 197, 768]               0\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "     ResidualAdd-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110            [-1, 197, 3072]       2,362,368\n",
      "            GELU-111            [-1, 197, 3072]               0\n",
      "         Dropout-112            [-1, 197, 3072]               0\n",
      "          Linear-113             [-1, 197, 768]       2,360,064\n",
      "         Dropout-114             [-1, 197, 768]               0\n",
      "     ResidualAdd-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118          [-1, 8, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-120             [-1, 197, 768]               0\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "     ResidualAdd-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "     ResidualAdd-129             [-1, 197, 768]               0\n",
      "       LayerNorm-130             [-1, 197, 768]           1,536\n",
      "          Linear-131            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-132          [-1, 8, 197, 197]               0\n",
      "          Linear-133             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-134             [-1, 197, 768]               0\n",
      "         Dropout-135             [-1, 197, 768]               0\n",
      "     ResidualAdd-136             [-1, 197, 768]               0\n",
      "       LayerNorm-137             [-1, 197, 768]           1,536\n",
      "          Linear-138            [-1, 197, 3072]       2,362,368\n",
      "            GELU-139            [-1, 197, 3072]               0\n",
      "         Dropout-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "     ResidualAdd-143             [-1, 197, 768]               0\n",
      "       LayerNorm-144             [-1, 197, 768]           1,536\n",
      "          Linear-145            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-146          [-1, 8, 197, 197]               0\n",
      "          Linear-147             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-148             [-1, 197, 768]               0\n",
      "         Dropout-149             [-1, 197, 768]               0\n",
      "     ResidualAdd-150             [-1, 197, 768]               0\n",
      "       LayerNorm-151             [-1, 197, 768]           1,536\n",
      "          Linear-152            [-1, 197, 3072]       2,362,368\n",
      "            GELU-153            [-1, 197, 3072]               0\n",
      "         Dropout-154            [-1, 197, 3072]               0\n",
      "          Linear-155             [-1, 197, 768]       2,360,064\n",
      "         Dropout-156             [-1, 197, 768]               0\n",
      "     ResidualAdd-157             [-1, 197, 768]               0\n",
      "       LayerNorm-158             [-1, 197, 768]           1,536\n",
      "          Linear-159            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-160          [-1, 8, 197, 197]               0\n",
      "          Linear-161             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-162             [-1, 197, 768]               0\n",
      "         Dropout-163             [-1, 197, 768]               0\n",
      "     ResidualAdd-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 3072]       2,362,368\n",
      "            GELU-167            [-1, 197, 3072]               0\n",
      "         Dropout-168            [-1, 197, 3072]               0\n",
      "          Linear-169             [-1, 197, 768]       2,360,064\n",
      "         Dropout-170             [-1, 197, 768]               0\n",
      "     ResidualAdd-171             [-1, 197, 768]               0\n",
      "          Reduce-172                  [-1, 768]               0\n",
      "       LayerNorm-173                  [-1, 768]           1,536\n",
      "          Linear-174                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 364.33\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 694.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from einops.layers.torch import Reduce\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int=12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, embedding_size: int=768,\n",
    "                 n_classes: int=1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(embedding_size),\n",
    "            nn.Linear(embedding_size, n_classes)\n",
    "        )\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 input_channels: int=3,\n",
    "                 patch_size: int=16,\n",
    "                 embedding_size: int=768,\n",
    "                 img_size: int=224,\n",
    "                 depth: int=12,\n",
    "                 n_classes: int=1000, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(input_channels, patch_size, embedding_size, img_size),\n",
    "            TransformerEncoder(depth, embedding_size=embedding_size, **kwargs),\n",
    "            ClassificationHead(embedding_size, n_classes)\n",
    "        )\n",
    "\n",
    "summary(ViT(), (3, 224, 224), device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
